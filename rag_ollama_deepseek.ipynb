{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/ai-agent-insider/developing-rag-systems-with-deepseek-r1-ollama-66a520bf0b88\n",
    "\n",
    "https://medium.com/@himeltasrif/run-deepseek-r1-locally-build-a-custom-vector-database-ai-chatbot-with-ollama-faiss-291ec9fe6ecf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import logging\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, model_name: str = \"llama2:7b-chat-q4\", max_memory_gb: float = 3.0):\n",
    "        self.setup_logging()\n",
    "        self.check_system_memory(max_memory_gb)\n",
    "        \n",
    "        # Load the language model (LLM)\n",
    "        self.llm = OllamaLLM(model=model_name,\n",
    "                            temperature=0.3,  # Lower creativity for concise responses\n",
    "                            top_p=0.85,       # Adjust diversity slightly\n",
    "                            max_tokens=150    # Limit response length\n",
    "                            )  \n",
    "        \n",
    "        # Initialize embeddings using a lightweight model\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "            model_kwargs={'device': 'cpu'}  # Use CPU for efficiency\n",
    "        )\n",
    "        \n",
    "        # Define the prompt template\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Answer the question based on the following context. Be concise.\n",
    "        Reason about the context to provide a well-thought-out answer.\n",
    "        If you cannot find the answer in the context, use your general knowledge to provide an answer.\n",
    "        \n",
    "        I'm giving you a document that contains the information about my spendings in the different months: in particular the \n",
    "        amount of money spent at the supermarket and the money for electricity.\n",
    "        \n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Answer: \"\"\")\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def check_system_memory(self, max_memory_gb: float):\n",
    "        available_memory = psutil.virtual_memory().available / (1024 ** 3)\n",
    "        self.logger.info(f\"Available system memory: {available_memory:.1f} GB\")\n",
    "        if available_memory < max_memory_gb:\n",
    "            self.logger.warning(\"Memory is below recommended threshold.\")\n",
    "            \n",
    "    def load_and_split_documents(self, file_path: str) -> List[Document]:\n",
    "        # Check if the file is a CSV or a TXT file and load accordingly\n",
    "        if file_path.endswith('.csv'):\n",
    "            loader = CSVLoader(file_path=file_path, csv_args={'delimiter': ';'})\n",
    "        elif file_path.endswith('.txt'):\n",
    "            loader = TextLoader(file_path=file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Please provide a .csv or .txt file.\")\n",
    "        documents = loader.load()\n",
    "        print(documents)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        splits = text_splitter.split_documents(documents)\n",
    "        self.logger.info(f\"Created {len(splits)} document chunks\")\n",
    "        return splits\n",
    "    \n",
    "    def create_vectorstore(self, documents: List[Document]) -> FAISS:\n",
    "        batch_size = 32\n",
    "        vectorstore = FAISS.from_documents(documents[:batch_size], self.embeddings)\n",
    "        \n",
    "        for i in range(batch_size, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            vectorstore.add_documents(batch)\n",
    "            self.logger.info(f\"Processed batch {i//batch_size + 1}\")\n",
    "        \n",
    "        # save locally the db \n",
    "        DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "        vectorstore.save_local(DB_FAISS_PATH)\n",
    "        \n",
    "        return vectorstore\n",
    "    \n",
    "    def setup_rag_chain(self, vectorstore: FAISS):\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3, \"fetch_k\": 1})\n",
    "        #retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"fetch_k\": 1})\n",
    "        \n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        \n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return rag_chain  \n",
    "    \n",
    "    def query(self, chain, question: str) -> str:\n",
    "        memory_usage = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "        self.logger.info(f\"Memory usage: {memory_usage:.1f} MB\")\n",
    "        return chain.invoke(question) \n",
    "    \n",
    "    def retrieval_qa_chain(self, db):\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 1}),  # Retrieve the most relevant document\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={'prompt': self.prompt}\n",
    "        )\n",
    "        return qa_chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    rag = RAGPipeline(model_name=\"deepseek-r1:1.5b\", max_memory_gb=3.0)\n",
    "    \n",
    "    #documents = rag.load_and_split_documents(\"trial.txt\")\n",
    "    documents = rag.load_and_split_documents(\"trial.csv\")\n",
    "    vectorstore = rag.create_vectorstore(documents)\n",
    "    chain = rag.setup_rag_chain(vectorstore)\n",
    "    \n",
    "    #question = \"What do you think about finance?\"\n",
    "    question = \"In which month did I spend the most money?\"\n",
    "    response = rag.query(chain, question)\n",
    "    print(f\"Question: {question}\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Available system memory: 12.1 GB\n",
      "/home/fabiofava98/Desktop_Ubuntu/AI/DeepSeek/venv_deepseek/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:__main__:Created 12 document chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'trial.csv', 'row': 0}, page_content='\\ufeffMESE: Gennaio\\nTOT SPESE SUPERMERCATO: 120\\nSPESA UTENZE DOMESTICHE: 50'), Document(metadata={'source': 'trial.csv', 'row': 1}, page_content='\\ufeffMESE: Febbraio\\nTOT SPESE SUPERMERCATO: 200\\nSPESA UTENZE DOMESTICHE: 200'), Document(metadata={'source': 'trial.csv', 'row': 2}, page_content='\\ufeffMESE: Marzo\\nTOT SPESE SUPERMERCATO: 300\\nSPESA UTENZE DOMESTICHE: 122'), Document(metadata={'source': 'trial.csv', 'row': 3}, page_content='\\ufeffMESE: Aprile\\nTOT SPESE SUPERMERCATO: 345\\nSPESA UTENZE DOMESTICHE: 34'), Document(metadata={'source': 'trial.csv', 'row': 4}, page_content='\\ufeffMESE: Maggio\\nTOT SPESE SUPERMERCATO: 50\\nSPESA UTENZE DOMESTICHE: 56'), Document(metadata={'source': 'trial.csv', 'row': 5}, page_content='\\ufeffMESE: Giugno\\nTOT SPESE SUPERMERCATO: 234\\nSPESA UTENZE DOMESTICHE: 23'), Document(metadata={'source': 'trial.csv', 'row': 6}, page_content='\\ufeffMESE: Luglio\\nTOT SPESE SUPERMERCATO: 133\\nSPESA UTENZE DOMESTICHE: 245'), Document(metadata={'source': 'trial.csv', 'row': 7}, page_content='\\ufeffMESE: Agosto\\nTOT SPESE SUPERMERCATO: 241\\nSPESA UTENZE DOMESTICHE: 21'), Document(metadata={'source': 'trial.csv', 'row': 8}, page_content='\\ufeffMESE: Settembre\\nTOT SPESE SUPERMERCATO: 213\\nSPESA UTENZE DOMESTICHE: 24'), Document(metadata={'source': 'trial.csv', 'row': 9}, page_content='\\ufeffMESE: Ottobre\\nTOT SPESE SUPERMERCATO: 123\\nSPESA UTENZE DOMESTICHE: 24'), Document(metadata={'source': 'trial.csv', 'row': 10}, page_content='\\ufeffMESE: Novembre\\nTOT SPESE SUPERMERCATO: 132\\nSPESA UTENZE DOMESTICHE: 342'), Document(metadata={'source': 'trial.csv', 'row': 11}, page_content='\\ufeffMESE: Dicembre\\nTOT SPESE SUPERMERCATO: 321\\nSPESA UTENZE DOMESTICHE: 12')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:__main__:Memory usage: 1082.6 MB\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In which month did I spend the most money?\n",
      "Answer: <think>\n",
      "Okay, so I need to figure out in which month I spent the most money based on the given context. Let me start by reading through the information carefully.\n",
      "\n",
      "First, there's a table with months and two columns: \"MESE\" (which translates to \"Month\") and \"TOT SPESE SUPERMERCATO.\" The third column seems to be \"SPESA UTENZE DOMESTICHE,\" which I think stands for \"Domestic Expenses.\"\n",
      "\n",
      "Looking at each month:\n",
      "\n",
      "- In April, TOT SPESE is 345 and Domestic Expenses are 34.\n",
      "- In November, TOT SPESE is 132 and Domestic Expenses are 342.\n",
      "- In February, both TOT SPESE and Domestic Expenses are 200.\n",
      "\n",
      "Now, I need to compare the total expenses for each month. April has a higher total of 345 compared to November's 132 and February's 200. So, April must be where the most money was spent.\n",
      "</think>\n",
      "\n",
      "April\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_bot():\n",
    "    rag = RAGPipeline(model_name=\"deepseek-r1:1.5b\", max_memory_gb=3.0)\n",
    "    \n",
    "    #documents = rag.load_and_split_documents(\"trial.txt\")\n",
    "    documents = rag.load_and_split_documents(\"trial.csv\")\n",
    "    vectorstore = rag.create_vectorstore(documents)\n",
    "    qa = rag.retrieval_qa_chain(vectorstore)\n",
    "    \n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    chain = qa_bot()\n",
    "    msg = cl.Message(content=\"Starting the bot...\")\n",
    "    await msg.send()\n",
    "    msg.content = \"Hi, Welcome to the DeepSeek ChatBot. How can I assist you today?\"\n",
    "    await msg.update()\n",
    "    cl.user_session.set(\"chain\", chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    chain = cl.user_session.get(\"chain\")\n",
    "    if chain is None:\n",
    "        await cl.Message(content=\"Error: Chain not initialized.\").send()\n",
    "        return\n",
    "\n",
    "    cb = cl.AsyncLangchainCallbackHandler(\n",
    "        stream_final_answer=True,\n",
    "        answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "    )\n",
    "\n",
    "    response = await chain.acall({'query': message.content}, callbacks=[cb])\n",
    "    answer = response[\"result\"]\n",
    "    sources = response.get(\"source_documents\", [])\n",
    "\n",
    "    if sources:\n",
    "        answer += \"\\nSources:\" + \"\\n\".join([str(doc.metadata['source']) for doc in sources])\n",
    "    else:\n",
    "        answer += \"\\nNo sources found\"\n",
    "\n",
    "    await cl.Message(content=answer).send()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
